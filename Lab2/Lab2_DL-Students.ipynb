{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> Sara Giammusso, Samuel Pierre\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288541596\n",
      "Epoch:  02   =====> Loss= 0.732278796\n",
      "Epoch:  03   =====> Loss= 0.600171506\n",
      "Epoch:  04   =====> Loss= 0.536730367\n",
      "Epoch:  05   =====> Loss= 0.497923466\n",
      "Epoch:  06   =====> Loss= 0.471160007\n",
      "Epoch:  07   =====> Loss= 0.451231807\n",
      "Epoch:  08   =====> Loss= 0.436027303\n",
      "Epoch:  09   =====> Loss= 0.423226904\n",
      "Epoch:  10   =====> Loss= 0.413144688\n",
      "Epoch:  11   =====> Loss= 0.404152078\n",
      "Epoch:  12   =====> Loss= 0.396842940\n",
      "Epoch:  13   =====> Loss= 0.390156472\n",
      "Epoch:  14   =====> Loss= 0.384509318\n",
      "Epoch:  15   =====> Loss= 0.379344410\n",
      "Epoch:  16   =====> Loss= 0.374645741\n",
      "Epoch:  17   =====> Loss= 0.369958724\n",
      "Epoch:  18   =====> Loss= 0.366439914\n",
      "Epoch:  19   =====> Loss= 0.362669270\n",
      "Epoch:  20   =====> Loss= 0.359825992\n",
      "Epoch:  21   =====> Loss= 0.356582394\n",
      "Epoch:  22   =====> Loss= 0.353991706\n",
      "Epoch:  23   =====> Loss= 0.351375227\n",
      "Epoch:  24   =====> Loss= 0.348629794\n",
      "Epoch:  25   =====> Loss= 0.346490239\n",
      "Epoch:  26   =====> Loss= 0.344349610\n",
      "Epoch:  27   =====> Loss= 0.342315207\n",
      "Epoch:  28   =====> Loss= 0.340254379\n",
      "Epoch:  29   =====> Loss= 0.338441756\n",
      "Epoch:  30   =====> Loss= 0.336789791\n",
      "Epoch:  31   =====> Loss= 0.335114212\n",
      "Epoch:  32   =====> Loss= 0.333316406\n",
      "Epoch:  33   =====> Loss= 0.332125139\n",
      "Epoch:  34   =====> Loss= 0.330548071\n",
      "Epoch:  35   =====> Loss= 0.329151955\n",
      "Epoch:  36   =====> Loss= 0.327981622\n",
      "Epoch:  37   =====> Loss= 0.326657533\n",
      "Epoch:  38   =====> Loss= 0.325241813\n",
      "Epoch:  39   =====> Loss= 0.324284867\n",
      "Epoch:  40   =====> Loss= 0.323210103\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9163\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions  for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride, padding_):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model(data, transfer=\"ReLU\", keep_prob=1.):    \n",
    "    \n",
    "    transferFuncs = {\"sigmoid\" : tf.sigmoid, \"ReLU\": tf.nn.relu}\n",
    "        \n",
    "    #first convolutional layer\n",
    "    W_conv1 = weight_variable([5, 5, 1, 6]) ## [filter_width, filter_height, depth_image_in, depth_image_out]\n",
    "    b_conv1 = bias_variable([6])\n",
    "    h_conv1 = transferFuncs[transfer](conv2d(data, W_conv1, 1, 'SAME') + b_conv1)\n",
    "    pool1 = tf.nn.pool(h_conv1, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #second convolutional layer\n",
    "    W_conv2 = weight_variable([5, 5, 6, 16])\n",
    "    b_conv2 = bias_variable([16])\n",
    "    h_conv2 = transferFuncs[transfer](conv2d(pool1, W_conv2, 1, 'VALID') + b_conv2)\n",
    "    pool2 = tf.nn.pool(h_conv2, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #first fully connected layer\n",
    "    s = pool2.get_shape().as_list()\n",
    "    flattened_length = s[1] * s[2] * s[3]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, flattened_length])\n",
    "    W_fc1 = weight_variable([1*5*5*16, 120])\n",
    "    b_fc1 = bias_variable([120])\n",
    "    h_fc1 = transferFuncs[transfer](tf.matmul(pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    #second fully connected layer\n",
    "    W_fc2 = weight_variable([120, 84])\n",
    "    b_fc2 = bias_variable([84])\n",
    "    h_fc2 = transferFuncs[transfer](tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "    \n",
    "    #third fully connected layer\n",
    "    W_fc3 = weight_variable([84, 10])\n",
    "    b_fc3 = bias_variable([10])\n",
    "    h_fc3 = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "    \n",
    "    return h_fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  61706\n"
     ]
    }
   ],
   "source": [
    "# first conv\n",
    "pconv1 = 5*5*1*6 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# second conv\n",
    "pconv2 = 5*5*6*16 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# first fcl\n",
    "pfcl1 = 5*5*16*120 # fcl_input_size * fcl_output_size\n",
    "# second fcl\n",
    "pfcl2 = 84*120 # fcl_input_size * fcl_output_size\n",
    "# third fcl\n",
    "pfcl3 = 84*10 # fcl_input_size * fcl_output_size\n",
    "# all the biases\n",
    "pbias = 6 + 16 + 120 + 84 + 10 \n",
    "\n",
    "total = pbias + pfcl1 + pfcl2 + pfcl3 + pconv2 + pconv1\n",
    "print('Total number of parameters: ', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "logs_path = 'log_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, y):\n",
    "    correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def train(learning_rate, training_epochs, batch_size, display_step = 1, \\\n",
    "          logs_path='log_files/', optFunction=\"SGD\", verbose=True, transfer=\"ReLU\", keep_probability= 1.0):\n",
    "    \n",
    "    optFunctions = {\"SGD\":tf.train.GradientDescentOptimizer, \"Adam\":tf.train.AdamOptimizer}\n",
    "    \n",
    "    # Erase previous graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Construct model\n",
    "    with tf.name_scope('Model'):\n",
    "        pred = LeNet5_Model(x, transfer=transfer)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "    with tf.name_scope(optFunction):\n",
    "        if transfer is \"sigmoid\":\n",
    "            optimizer = optFunctions[optFunction](learning_rate).minimize(cost)\n",
    "        else:\n",
    "            opt = optFunctions[optFunction](learning_rate)\n",
    "            gvs = opt.compute_gradients(cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "            optimizer = opt.apply_gradients(capped_gvs)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = evaluate(pred, y)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    x_val, y_val = mnist.validation.images.reshape(-1, 28, 28, 1), mnist.validation.labels\n",
    "    x_test, y_test = mnist.test.images.reshape(-1, 28, 28, 1), mnist.test.labels\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        #acc_history = []\n",
    "        test_history = []\n",
    "        val_history = []\n",
    "        #train_history = []\n",
    "\n",
    "        sess.run(init)\n",
    "        if verbose is True:\n",
    "            print(\"Start Training!\")\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        # Training cycle\n",
    "        start_time = time.time()\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle = True)\n",
    "                batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys, keep_prob: keep_probability})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            #train_acc = accuracy.eval({x: batch_xs, y:batch_ys})\n",
    "            val_acc = accuracy.eval({x: x_val, y:y_val, keep_prob:keep_probability})\n",
    "            test_acc = accuracy.eval({x: x_test, y:y_test, keep_prob:keep_probability})\n",
    "\n",
    "            #train_history.append(train_acc)\n",
    "            val_history.append(val_acc)\n",
    "            test_history.append(test_acc)\n",
    "            \n",
    "            \n",
    "            if verbose is True and (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \\\n",
    "                      \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                      \" Validation accuracy=\", val_acc, \" Test accuracy=\", test_acc)\n",
    "            if test_acc >= 0.99:\n",
    "                if verbose is True:\n",
    "                    print(\"Test Accuracy over 99%% reached after %d epochs\" %(epoch+1))\n",
    "                break\n",
    "        elapsed_time = time.time() - start_time \n",
    "        saver.save(sess, 'Models/model_' + str(learning_rate) + '_' + str(batch_size) + '_' + optFunction)\n",
    "        if verbose is True:\n",
    "            print(\"Training Finished!\")\n",
    "            # Test model\n",
    "            # Calculate accuracy\n",
    "            print(\"Test accuracy:\", accuracy.eval({x: x_test, y:y_test, keep_prob: keep_probability}))\n",
    "            print(\"Elapsed time: \", elapsed_time, \"sec\")\n",
    "        \n",
    "    return val_history, test_history, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 2.280956376  Validation accuracy= 0.2522  Test accuracy= 0.254\n",
      "Epoch:  02   =====> Loss= 2.206633648  Validation accuracy= 0.4266  Test accuracy= 0.4146\n",
      "Epoch:  03   =====> Loss= 2.066152951  Validation accuracy= 0.5284  Test accuracy= 0.5282\n",
      "Epoch:  04   =====> Loss= 1.724710685  Validation accuracy= 0.6536  Test accuracy= 0.664\n",
      "Epoch:  05   =====> Loss= 1.194991675  Validation accuracy= 0.772  Test accuracy= 0.7696\n",
      "Epoch:  06   =====> Loss= 0.802881073  Validation accuracy= 0.8238  Test accuracy= 0.8239\n",
      "Epoch:  07   =====> Loss= 0.610041574  Validation accuracy= 0.8544  Test accuracy= 0.8544\n",
      "Epoch:  08   =====> Loss= 0.507497538  Validation accuracy= 0.868  Test accuracy= 0.8701\n",
      "Epoch:  09   =====> Loss= 0.449256792  Validation accuracy= 0.8856  Test accuracy= 0.8854\n",
      "Epoch:  10   =====> Loss= 0.407330606  Validation accuracy= 0.8912  Test accuracy= 0.8958\n",
      "Epoch:  11   =====> Loss= 0.377987391  Validation accuracy= 0.9008  Test accuracy= 0.901\n",
      "Epoch:  12   =====> Loss= 0.358906873  Validation accuracy= 0.9058  Test accuracy= 0.9048\n",
      "Epoch:  13   =====> Loss= 0.339783259  Validation accuracy= 0.9088  Test accuracy= 0.9088\n",
      "Epoch:  14   =====> Loss= 0.323461757  Validation accuracy= 0.9114  Test accuracy= 0.9131\n",
      "Epoch:  15   =====> Loss= 0.310931986  Validation accuracy= 0.9178  Test accuracy= 0.9156\n",
      "Epoch:  16   =====> Loss= 0.298689609  Validation accuracy= 0.9176  Test accuracy= 0.9204\n",
      "Epoch:  17   =====> Loss= 0.287800368  Validation accuracy= 0.9222  Test accuracy= 0.9217\n",
      "Epoch:  18   =====> Loss= 0.278617123  Validation accuracy= 0.9238  Test accuracy= 0.9243\n",
      "Epoch:  19   =====> Loss= 0.267595175  Validation accuracy= 0.926  Test accuracy= 0.9276\n",
      "Epoch:  20   =====> Loss= 0.262448333  Validation accuracy= 0.9282  Test accuracy= 0.9281\n",
      "Epoch:  21   =====> Loss= 0.255444207  Validation accuracy= 0.929  Test accuracy= 0.9306\n",
      "Epoch:  22   =====> Loss= 0.244516066  Validation accuracy= 0.9326  Test accuracy= 0.9326\n",
      "Epoch:  23   =====> Loss= 0.242154158  Validation accuracy= 0.9348  Test accuracy= 0.9335\n",
      "Epoch:  24   =====> Loss= 0.233470833  Validation accuracy= 0.938  Test accuracy= 0.9365\n",
      "Epoch:  25   =====> Loss= 0.229033125  Validation accuracy= 0.9394  Test accuracy= 0.9377\n",
      "Epoch:  26   =====> Loss= 0.220721268  Validation accuracy= 0.9384  Test accuracy= 0.939\n",
      "Epoch:  27   =====> Loss= 0.216331457  Validation accuracy= 0.9406  Test accuracy= 0.9425\n",
      "Epoch:  28   =====> Loss= 0.211064333  Validation accuracy= 0.9436  Test accuracy= 0.9413\n",
      "Epoch:  29   =====> Loss= 0.206995926  Validation accuracy= 0.9456  Test accuracy= 0.9448\n",
      "Epoch:  30   =====> Loss= 0.202928415  Validation accuracy= 0.9432  Test accuracy= 0.9451\n",
      "Epoch:  31   =====> Loss= 0.197818024  Validation accuracy= 0.9478  Test accuracy= 0.9473\n",
      "Epoch:  32   =====> Loss= 0.193193208  Validation accuracy= 0.9478  Test accuracy= 0.9476\n",
      "Epoch:  33   =====> Loss= 0.189609538  Validation accuracy= 0.95  Test accuracy= 0.9494\n",
      "Epoch:  34   =====> Loss= 0.186120406  Validation accuracy= 0.9506  Test accuracy= 0.9492\n",
      "Epoch:  35   =====> Loss= 0.181786983  Validation accuracy= 0.9504  Test accuracy= 0.9506\n",
      "Epoch:  36   =====> Loss= 0.178504138  Validation accuracy= 0.9528  Test accuracy= 0.9513\n",
      "Epoch:  37   =====> Loss= 0.172617774  Validation accuracy= 0.9552  Test accuracy= 0.9521\n",
      "Epoch:  38   =====> Loss= 0.175305551  Validation accuracy= 0.9542  Test accuracy= 0.9529\n",
      "Epoch:  39   =====> Loss= 0.167101085  Validation accuracy= 0.9564  Test accuracy= 0.9537\n",
      "Epoch:  40   =====> Loss= 0.165871229  Validation accuracy= 0.956  Test accuracy= 0.9541\n",
      "Training Finished!\n",
      "Test accuracy: 0.9541\n",
      "Elapsed time:  653.0051808357239 sec\n"
     ]
    }
   ],
   "source": [
    "val_hist_1, test_hist_1, elapsed_time_1 = train(learning_rate, training_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please put your loss and accuracy curves here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |       95.41%       |        99%          |       \n",
    "| Training Time        |       11 min       |        5 min        |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "**The optimizer that gives the best accuracy on test data is AdamOptimizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 0.386171725  Validation accuracy= 0.9644  Test accuracy= 0.9647\n",
      "Epoch:  02   =====> Loss= 0.092860161  Validation accuracy= 0.9804  Test accuracy= 0.9811\n",
      "Epoch:  03   =====> Loss= 0.065752518  Validation accuracy= 0.982  Test accuracy= 0.981\n",
      "Epoch:  04   =====> Loss= 0.051985543  Validation accuracy= 0.9852  Test accuracy= 0.9868\n",
      "Epoch:  05   =====> Loss= 0.042831245  Validation accuracy= 0.9894  Test accuracy= 0.9874\n",
      "Epoch:  06   =====> Loss= 0.037709890  Validation accuracy= 0.9852  Test accuracy= 0.9883\n",
      "Epoch:  07   =====> Loss= 0.032009845  Validation accuracy= 0.987  Test accuracy= 0.9857\n",
      "Epoch:  08   =====> Loss= 0.028669685  Validation accuracy= 0.9872  Test accuracy= 0.9895\n",
      "Epoch:  09   =====> Loss= 0.025506127  Validation accuracy= 0.987  Test accuracy= 0.9897\n",
      "Epoch:  10   =====> Loss= 0.022337955  Validation accuracy= 0.9884  Test accuracy= 0.9895\n",
      "Epoch:  11   =====> Loss= 0.019967023  Validation accuracy= 0.9846  Test accuracy= 0.9853\n",
      "Epoch:  12   =====> Loss= 0.017275304  Validation accuracy= 0.9898  Test accuracy= 0.9885\n",
      "Epoch:  13   =====> Loss= 0.017381820  Validation accuracy= 0.9886  Test accuracy= 0.9886\n",
      "Epoch:  14   =====> Loss= 0.012676073  Validation accuracy= 0.9874  Test accuracy= 0.989\n",
      "Epoch:  15   =====> Loss= 0.011891883  Validation accuracy= 0.989  Test accuracy= 0.9899\n",
      "Epoch:  16   =====> Loss= 0.012360548  Validation accuracy= 0.989  Test accuracy= 0.9898\n",
      "Epoch:  17   =====> Loss= 0.010232390  Validation accuracy= 0.9888  Test accuracy= 0.9906\n",
      "Test Accuracy over 99% reached after 17 epochs\n",
      "Training Finished!\n",
      "Test accuracy: 0.9906\n",
      "Elapsed time:  277.0475277900696 sec\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "val_hist_2, test_hist_2, elapsed_time_2 = train(learning_rate, training_epochs, batch_size, optFunction=\"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 0.377899568  Validation accuracy= 0.9682  Test accuracy= 0.9666\n",
      "Epoch:  02   =====> Loss= 0.088502849  Validation accuracy= 0.9786  Test accuracy= 0.9794\n",
      "Epoch:  03   =====> Loss= 0.060588664  Validation accuracy= 0.9836  Test accuracy= 0.9865\n",
      "Epoch:  04   =====> Loss= 0.046992287  Validation accuracy= 0.9842  Test accuracy= 0.9849\n",
      "Epoch:  05   =====> Loss= 0.037397350  Validation accuracy= 0.9872  Test accuracy= 0.9889\n",
      "Epoch:  06   =====> Loss= 0.030449494  Validation accuracy= 0.9864  Test accuracy= 0.9879\n",
      "Epoch:  07   =====> Loss= 0.026594427  Validation accuracy= 0.9862  Test accuracy= 0.9878\n",
      "Epoch:  08   =====> Loss= 0.023228382  Validation accuracy= 0.9884  Test accuracy= 0.9908\n",
      "Test Accuracy over 99% reached after 8 epochs\n",
      "Training Finished!\n",
      "Test accuracy: 0.9908\n",
      "Elapsed time:  130.06636428833008 sec\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "val_hist_3, test_hist_3, elapsed_time_3 = train(learning_rate, training_epochs, batch_size, optFunction=\"Adam\", keep_probability= 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this configuration, we are able to reach 99% accuracy in a shorter time. Dropout is a useful technique and we see it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
